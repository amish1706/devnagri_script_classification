{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Importing libraries\r\n",
    "import tensorflow as tf\r\n",
    "import cv2\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# loading classes\r\n",
    "classes = os.listdir(\"./Data/Train\")\r\n",
    "print(classes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['character_10_yna', 'character_11_taamatar', 'character_12_thaa', 'character_13_daa', 'character_14_dhaa', 'character_15_adna', 'character_16_tabala', 'character_17_tha', 'character_18_da', 'character_19_dha', 'character_1_ka', 'character_20_na', 'character_21_pa', 'character_22_pha', 'character_23_ba', 'character_24_bha', 'character_25_ma', 'character_26_yaw', 'character_27_ra', 'character_28_la', 'character_29_waw', 'character_2_kha', 'character_30_motosaw', 'character_31_petchiryakha', 'character_32_patalosaw', 'character_33_ha', 'character_34_chhya', 'character_35_tra', 'character_36_gya', 'character_3_ga', 'character_4_gha', 'character_5_kna', 'character_6_cha', 'character_7_chha', 'character_8_ja', 'character_9_jha', 'digit_0', 'digit_1', 'digit_2', 'digit_3', 'digit_4', 'digit_5', 'digit_6', 'digit_7', 'digit_8', 'digit_9']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# loading training image data into an array using openCV\r\n",
    "X = []\r\n",
    "for c,i in enumerate(classes):\r\n",
    "    for j in os.listdir(\"./Data/Train/\"+i):\r\n",
    "        img = cv2.imread(\"./Data/Train/\"+i+\"/\"+j)\r\n",
    "        X.append(img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# loading target training data\r\n",
    "Y = []\r\n",
    "for i in range(0,len(X)):\r\n",
    "    Y.append(i//1700)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "X = np.array(X)\r\n",
    "Y = np.array(Y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "Y.reshape(-1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 45, 45, 45])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# shuffling the dataset\r\n",
    "from sklearn.utils import shuffle\r\n",
    "X, Y = shuffle(X, Y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X = np.array(X)\r\n",
    "Y = np.array(Y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "Y.reshape(-1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([35, 40, 26, ..., 14,  1, 11])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# visualizing the dataset\r\n",
    "no = 2\r\n",
    "plt.imshow(X[no])\r\n",
    "plt.xlabel(classes[Y[no]])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'character_34_chhya')"
      ]
     },
     "metadata": {},
     "execution_count": 12
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# train-test split\r\n",
    "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size=0.05, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "train_y.reshape(-1)\r\n",
    "val_y.reshape(-1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([35,  2,  0, ..., 33, 13, 15])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 46 total classes\r\n",
    "NUM_CLASSES = 46"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", update_freq='batch')\r\n",
    "# tensorboard --logdir=\"E:\\Amish\\CnA Hackathon\\logs\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# importing ResNet34 model architecture\r\n",
    "from ResNet import ResNet34\r\n",
    "model_res = ResNet34(NUM_CLASSES)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# defining Early Stopping callback to stop traing when the val_accuracy becomes greater than a certain value\r\n",
    "class EarlyStopping(tf.keras.callbacks.Callback):\r\n",
    "    def __init__(self, monitor='val_accuracy', value=0.995, verbose=0):\r\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\r\n",
    "        self.monitor = monitor\r\n",
    "        self.value = value\r\n",
    "        self.verbose = verbose\r\n",
    "\r\n",
    "    def on_epoch_end(self, epoch, logs={}):\r\n",
    "        current = logs.get(self.monitor)\r\n",
    "       \r\n",
    "        if current > self.value:\r\n",
    "            print(\"Epoch %05d: early stopping THR\" % epoch)\r\n",
    "            self.model.stop_training = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# compiling the model\r\n",
    "model_res.compile(loss='sparse_categorical_crossentropy', \r\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \r\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy(), 'accuracy'])\r\n",
    "# training the model for 10 epochs\r\n",
    "model_res.fit(train_x, train_y, epochs=10,\r\n",
    "          callbacks=[tensorboard_callback,EarlyStopping()],\r\n",
    "          validation_data=(val_x, val_y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "2322/2322 [==============================] - 277s 117ms/step - loss: 0.7027 - categorical_accuracy: 0.0223 - accuracy: 0.8011 - val_loss: 0.4179 - val_categorical_accuracy: 0.0174 - val_accuracy: 0.8967\n",
      "Epoch 2/10\n",
      "2322/2322 [==============================] - 290s 125ms/step - loss: 0.1064 - categorical_accuracy: 0.0209 - accuracy: 0.9678 - val_loss: 0.1268 - val_categorical_accuracy: 0.0251 - val_accuracy: 0.9611\n",
      "Epoch 3/10\n",
      "2322/2322 [==============================] - 266s 115ms/step - loss: 0.0703 - categorical_accuracy: 0.0209 - accuracy: 0.9792 - val_loss: 0.0935 - val_categorical_accuracy: 0.0256 - val_accuracy: 0.9772\n",
      "Epoch 4/10\n",
      "2322/2322 [==============================] - 325s 140ms/step - loss: 0.0524 - categorical_accuracy: 0.0215 - accuracy: 0.9847 - val_loss: 0.0615 - val_categorical_accuracy: 0.0274 - val_accuracy: 0.9831\n",
      "Epoch 5/10\n",
      "2322/2322 [==============================] - 337s 145ms/step - loss: 0.0387 - categorical_accuracy: 0.0214 - accuracy: 0.9880 - val_loss: 0.0792 - val_categorical_accuracy: 0.0258 - val_accuracy: 0.9788\n",
      "Epoch 6/10\n",
      "2322/2322 [==============================] - 264s 114ms/step - loss: 0.0327 - categorical_accuracy: 0.0213 - accuracy: 0.9902 - val_loss: 0.0873 - val_categorical_accuracy: 0.0274 - val_accuracy: 0.9780\n",
      "Epoch 7/10\n",
      "2322/2322 [==============================] - 263s 113ms/step - loss: 0.0236 - categorical_accuracy: 0.0222 - accuracy: 0.9935 - val_loss: 0.0437 - val_categorical_accuracy: 0.0261 - val_accuracy: 0.9870\n",
      "Epoch 8/10\n",
      "2322/2322 [==============================] - 271s 117ms/step - loss: 0.0190 - categorical_accuracy: 0.0207 - accuracy: 0.9942 - val_loss: 0.0416 - val_categorical_accuracy: 0.0258 - val_accuracy: 0.9895\n",
      "Epoch 9/10\n",
      "2322/2322 [==============================] - 272s 117ms/step - loss: 0.0161 - categorical_accuracy: 0.0216 - accuracy: 0.9950 - val_loss: 0.1110 - val_categorical_accuracy: 0.0258 - val_accuracy: 0.9760\n",
      "Epoch 10/10\n",
      "2322/2322 [==============================] - 264s 114ms/step - loss: 0.0160 - categorical_accuracy: 0.0229 - accuracy: 0.9960 - val_loss: 0.0360 - val_categorical_accuracy: 0.0256 - val_accuracy: 0.9905\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20dc3e4c460>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Evaluting on the validation dataset\r\n",
    "model_res.evaluate(val_x, val_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "123/123 [==============================] - 4s 30ms/step - loss: 0.0360 - categorical_accuracy: 0.0256 - accuracy: 0.9905\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.03604639694094658, 0.02557544782757759, 0.9905371069908142]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "model_res.save(\"resnet_model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: resnet_model\\assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# loading the test images\r\n",
    "X_test = []\r\n",
    "for dirs in os.listdir(\"./Data/Test\"):\r\n",
    "    X_test.append(cv2.imread(\"./Data/Test/\"+ dirs))\r\n",
    "X_test = np.array(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# getting predictions\r\n",
    "ypred = model_res.predict(X_test)\r\n",
    "# storing the predictions after converting then into class names\r\n",
    "y_pred = []\r\n",
    "for i in range(len(X_test)):\r\n",
    "    y_pred.append(np.argmax(ypred[i]))\r\n",
    "sub = [] \r\n",
    "i = 0\r\n",
    "for dirs in os.listdir(\"./Data/Test\"):\r\n",
    "    sub.append((dirs, classes[y_pred[i]]))\r\n",
    "    i+=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# saving the predictions in csv file\r\n",
    "submission = pd.DataFrame(sub)\r\n",
    "submission = submission.rename(columns = {1:\"Category\", 0:\"Id\"})\r\n",
    "submission.set_index('Id', inplace=True)\r\n",
    "submission.to_csv(\"submission.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tf2-gpu': conda)"
  },
  "interpreter": {
   "hash": "ce9241549650588f234d12c02b91e0d3550d6f6cc7b1eea88e70ed4e9777c474"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}